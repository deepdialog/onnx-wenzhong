{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'IDEA-CCNL/Wenzhong-GPT2-3.5B'\n",
    "model_name = './model'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"北京位于\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sample_output = model.generate(\n",
    "    encoded_input['input_ids'],\n",
    "    do_sample=True, \n",
    "    max_length=100, \n",
    "    top_k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'北京位于北温带，地势由西北向东南倾斜。\\n相对偏西的气流进入北京，会给降雨带来明显的增温作用，�'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"北京位于南城的协和医院\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "right_output = model(input_ids=encoded_input['input_ids'])\n",
    "right_logits = right_output.logits[:, -1, :].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.22758223116397858\n",
      "1 0.42101413011550903\n",
      "2 0.11645098030567169\n",
      "3 0.21743199229240417\n",
      "4 0.13183628022670746\n",
      "5 0.08746851980686188\n",
      "6 0.5519747734069824\n",
      "7 0.7463037371635437\n",
      "8 0.2039681226015091\n",
      "9 0.15316486358642578\n",
      "10 0.1427486538887024\n",
      "11 0.15914595127105713\n",
      "12 0.04127417132258415\n",
      "13 0.41353127360343933\n",
      "14 0.06513189524412155\n",
      "15 0.17800942063331604\n",
      "16 0.41627639532089233\n",
      "17 0.13674382865428925\n",
      "18 0.11395834386348724\n",
      "19 0.12362097948789597\n",
      "20 0.14189130067825317\n",
      "21 0.12765437364578247\n",
      "22 0.20745964348316193\n",
      "23 0.09250973165035248\n",
      "24 0.11203231662511826\n",
      "25 0.2353578507900238\n",
      "26 0.08013581484556198\n",
      "27 0.5328487753868103\n",
      "28 0.7309257984161377\n",
      "29 0.25862255692481995\n",
      "30 0.0751747414469719\n",
      "31 0.15293598175048828\n",
      "32 0.09560840576887131\n",
      "33 0.09238307923078537\n",
      "34 0.05506070330739021\n",
      "35 0.0653311237692833\n",
      "36 0.06226501241326332\n",
      "37 0.0709947943687439\n",
      "38 0.1473851054906845\n",
      "39 0.09755463898181915\n",
      "40 0.06169920414686203\n",
      "41 0.07941905409097672\n",
      "42 0.06480909883975983\n",
      "43 0.07727842032909393\n",
      "44 0.08654577285051346\n",
      "45 0.07057885825634003\n",
      "46 0.0949883908033371\n",
      "47 0.08020563423633575\n",
      "48 0.1592092514038086\n",
      "49 0.08128400146961212\n",
      "50207 0.5045775771141052\n",
      "50208 0.48096540570259094\n",
      "50209 0.5257660746574402\n",
      "50210 0.4737381041049957\n",
      "50211 0.5339730381965637\n",
      "50212 0.5819697976112366\n",
      "50213 0.582286536693573\n",
      "50214 0.45035138726234436\n",
      "50215 0.5558518767356873\n",
      "50216 0.5905548334121704\n",
      "50217 0.2733864486217499\n",
      "50218 0.3351300358772278\n",
      "50219 0.5509161949157715\n",
      "50220 0.44887328147888184\n",
      "50221 0.23186133801937103\n",
      "50222 0.5037680268287659\n",
      "50223 0.45799562335014343\n",
      "50224 0.4140816032886505\n",
      "50225 0.5253862738609314\n",
      "50226 0.48729151487350464\n",
      "50227 0.9941443800926208\n",
      "50228 0.3462538421154022\n",
      "50229 0.2689454257488251\n",
      "50230 0.572981059551239\n",
      "50231 0.3520515263080597\n",
      "50232 0.2603214979171753\n",
      "50233 0.49982917308807373\n",
      "50234 0.46736785769462585\n",
      "50235 0.20282875001430511\n",
      "50236 0.3047976791858673\n",
      "50237 0.3520374596118927\n",
      "50238 0.49316850304603577\n",
      "50239 0.521996796131134\n",
      "50240 0.5815914869308472\n",
      "50241 0.37967973947525024\n",
      "50242 0.11732432246208191\n",
      "50243 0.47905734181404114\n",
      "50244 0.408270925283432\n",
      "50245 0.5190487504005432\n",
      "50246 0.20359276235103607\n",
      "50247 0.532450795173645\n",
      "50248 0.3740718960762024\n",
      "50249 0.2676578462123871\n",
      "50250 0.5571309924125671\n",
      "50251 0.24768497049808502\n",
      "50252 0.3540804088115692\n",
      "50253 0.4530066251754761\n",
      "50254 0.5421429872512817\n",
      "50255 0.44492307305336\n",
      "50256 0.20508833229541779\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "diffs = []\n",
    "for i in range(50):\n",
    "    first_output = model(input_ids=torch.LongTensor([[i]]))\n",
    "    second_output = model(input_ids=encoded_input['input_ids'], past_key_values=first_output.past_key_values)\n",
    "    second_logits = second_output.logits[:, -1, :].view(-1)\n",
    "    diff = (torch.sum((second_logits - right_logits) ** 2) / len(right_logits)).detach().item()\n",
    "    print(i, diff)\n",
    "    diffs.append((i, diff))\n",
    "for i in range(tokenizer.vocab_size - 50, tokenizer.vocab_size):\n",
    "    first_output = model(input_ids=torch.LongTensor([[i]]))\n",
    "    second_output = model(input_ids=encoded_input['input_ids'], past_key_values=first_output.past_key_values)\n",
    "    second_logits = second_output.logits[:, -1, :].view(-1)\n",
    "    diff = (torch.sum((second_logits - right_logits) ** 2) / len(right_logits)).detach().item()\n",
    "    print(i, diff)\n",
    "    diffs.append((i, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = sorted(diffs, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 0.04127417132258415), (34, 0.05506070330739021), (40, 0.06169920414686203), (36, 0.06226501241326332), (42, 0.06480909883975983)]\n"
     ]
    }
   ],
   "source": [
    "print(diffs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivocab = {v: k for k, v in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivocab[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_output = model(input_ids=torch.LongTensor([[50256]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = first_output.past_key_values\n",
    "past_key_values = torch.stack([torch.stack(x) for x in past_key_values])\n",
    "past_key_values = past_key_values.detach().numpy()\n",
    "np.save('past_key_values', past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GPT, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids = None, past_key_values = None):\n",
    "        if past_key_values is not None:\n",
    "            past_key_values = torch.unbind(past_key_values)\n",
    "            past_key_values = [torch.unbind(x) for x in past_key_values]\n",
    "        out = self.model(input_ids=input_ids, past_key_values=past_key_values)\n",
    "        # [20, 2, 1, 16, x, 64]\n",
    "        # dim 4 is free\n",
    "        past_key_values = out.past_key_values\n",
    "        past_key_values = torch.stack([torch.stack(x) for x in past_key_values])\n",
    "\n",
    "        return out.logits, past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一\n",
      "一�\n",
      "一个\n",
      "一个�\n",
      "一个�\n",
      "一个普\n",
      "一个普�\n",
      "一个普通\n",
      "一个普通的\n",
      "一个普通的人\n",
      "一个普通的人�\n",
      "一个普通的人�\n",
      "一个普通的人，\n",
      "一个普通的人，�\n",
      "一个普通的人，我\n",
      "一个普通的人，我的\n",
      "一个普通的人，我的�\n",
      "一个普通的人，我的名\n",
      "一个普通的人，我的名�\n",
      "一个普通的人，我的名字\n",
      "一个普通的人，我的名字�\n",
      "一个普通的人，我的名字叫\n",
      "一个普通的人，我的名字叫AI\n",
      "一个普通的人，我的名字叫AI�\n",
      "一个普通的人，我的名字叫AI匠\n",
      "一个普通的人，我的名字叫AI匠�\n",
      "一个普通的人，我的名字叫AI匠�\n",
      "一个普通的人，我的名字叫AI匠，\n",
      "一个普通的人，我的名字叫AI匠，�\n",
      "一个普通的人，我的名字叫AI匠，我\n",
      "一个普通的人，我的名字叫AI匠，我的\n",
      "一个普通的人，我的名字叫AI匠，我的�\n",
      "一个普通的人，我的名字叫AI匠，我的名\n",
      "一个普通的人，我的名字叫AI匠，我的名�\n",
      "一个普通的人，我的名字叫AI匠，我的名字\n",
      "一个普通的人，我的名字叫AI匠，我的名字�\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI�\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠�\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠�\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，�\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，我\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，我的\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，我的�\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，我的名\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，我的名�\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，我的名字\n",
      "一个普通的人，我的名字叫AI匠，我的名字叫AI匠，我的名字�\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "output_seq = []\n",
    "logits, past_key_values = gpt(input_ids=encoded_input['input_ids'])\n",
    "next_token = int(np.argmax(logits[0, -1].detach().numpy()))\n",
    "output_seq.append(next_token)\n",
    "print(tokenizer.decode(output_seq))\n",
    "for i in range(50):\n",
    "    logits, past_key_values = gpt(input_ids=torch.LongTensor([[next_token]]), past_key_values=past_key_values)\n",
    "    next_token = int(np.argmax(logits[0, -1].detach().numpy()))\n",
    "    output_seq.append(next_token)\n",
    "    print(tokenizer.decode(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一\n",
      "一�\n",
      "一个\n",
      "一个�\n",
      "一个�\n",
      "一个普\n",
      "一个普�\n",
      "一个普通\n",
      "一个普通的\n",
      "一个普通的人\n",
      "一个普通的人�\n",
      "一个普通的人�\n",
      "一个普通的人，\n",
      "一个普通的人，�\n",
      "一个普通的人，我\n",
      "一个普通的人，我的\n",
      "一个普通的人，我的�\n",
      "一个普通的人，我的名\n",
      "一个普通的人，我的名�\n",
      "一个普通的人，我的名字\n",
      "一个普通的人，我的名字�\n",
      "一个普通的人，我的名字叫\n",
      "一个普通的人，我的名字叫AI\n",
      "一个普通的人，我的名字叫AI�\n",
      "一个普通的人，我的名字叫AI匠\n",
      "一个普通的人，我的名字叫AI匠�\n",
      "一个普通的人，我的名字叫AI匠�\n",
      "一个普通的人，我的名字叫AI匠，\n",
      "一个普通的人，我的名字叫AI匠，�\n",
      "一个普通的人，我的名字叫AI匠，我\n",
      "一个普通的人，我的名字叫AI匠，我是\n",
      "一个普通的人，我的名字叫AI匠，我是一\n",
      "一个普通的人，我的名字叫AI匠，我是一�\n",
      "一个普通的人，我的名字叫AI匠，我是一个\n",
      "一个普通的人，我的名字叫AI匠，我是一个�\n",
      "一个普通的人，我的名字叫AI匠，我是一个�\n",
      "一个普通的人，我的名字叫AI匠，我是一个普\n",
      "一个普通的人，我的名字叫AI匠，我是一个普�\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人�\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人�\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，�\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，我\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，我的\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，我的�\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，我的名\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，我的名�\n",
      "一个普通的人，我的名字叫AI匠，我是一个普通的人，我的名字\n"
     ]
    }
   ],
   "source": [
    "past_key_values_start = torch.zeros([30, 2, 1, 32, 1, 96])\n",
    "# first\n",
    "output_seq = []\n",
    "logits, past_key_values = gpt(input_ids=encoded_input['input_ids'], past_key_values=past_key_values_start)\n",
    "next_token = int(np.argmax(logits[0, -1].detach().numpy()))\n",
    "output_seq.append(next_token)\n",
    "print(tokenizer.decode(output_seq))\n",
    "for i in range(50):\n",
    "    logits, past_key_values = gpt(input_ids=torch.LongTensor([[next_token]]), past_key_values=past_key_values)\n",
    "    next_token = int(np.argmax(logits[0, -1].detach().numpy()))\n",
    "    output_seq.append(next_token)\n",
    "    print(tokenizer.decode(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start export\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('start export')\n",
    "!rm -rf onnx && mkdir -p onnx\n",
    "input_ids = torch.LongTensor([[1]])\n",
    "torch.onnx.export(\n",
    "    gpt,               # model being run\n",
    "    (input_ids, past_key_values_start),                         # model input (or a tuple for multiple inputs)\n",
    "    \"onnx/model.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=13,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input', 'pkv'],   # the model's input names\n",
    "    output_names = ['output', 'pkv_output'], # the model's output names\n",
    "    dynamic_axes={\n",
    "        'input' : {0 : 'batch_size', 1 : 'seq_len'},    # variable length axes\n",
    "        'output' : {0 : 'batch_size', 1 : 'seq_len'},\n",
    "        'pkv': {4: 'seq_len'},\n",
    "        'pkv_output': {4: 'seq_len'},\n",
    "    }\n",
    ")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_346]\n",
      "Ignore MatMul due to non constant B: /[MatMul_370]\n",
      "Ignore MatMul due to non constant B: /[MatMul_546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_570]\n",
      "Ignore MatMul due to non constant B: /[MatMul_746]\n",
      "Ignore MatMul due to non constant B: /[MatMul_770]\n",
      "Ignore MatMul due to non constant B: /[MatMul_946]\n",
      "Ignore MatMul due to non constant B: /[MatMul_970]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1146]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1170]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1346]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1370]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1570]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1746]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1770]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1946]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1970]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2146]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2170]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2346]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2370]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2570]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2746]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2770]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2946]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2970]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3146]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3170]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3346]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3370]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3570]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3746]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3770]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3946]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3970]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4146]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4170]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4346]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4370]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4570]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4746]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4770]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4946]\n",
      "Ignore MatMul due to non constant B: /[MatMul_4970]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5146]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5170]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5346]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5370]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5570]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5746]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5770]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5946]\n",
      "Ignore MatMul due to non constant B: /[MatMul_5970]\n",
      "Ignore MatMul due to non constant B: /[MatMul_6146]\n",
      "Ignore MatMul due to non constant B: /[MatMul_6170]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf onnxq && mkdir -p onnxq\n",
    "model_fp32 = 'onnx/model.onnx'\n",
    "model_quant = 'onnxq/model.onnx'\n",
    "quantized_model = quantize_dynamic(\n",
    "    model_fp32,\n",
    "    model_quant,\n",
    "    use_external_data_format=True,\n",
    "    weight_type=QuantType.QUInt8,  # 默认的QInt8会产生极大的误差\n",
    "    extra_options={\n",
    "        'DisableShapeInference': True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14G\tonnx\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5G\tonnxq\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh onnxq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
